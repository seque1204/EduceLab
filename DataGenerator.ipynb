{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDf23Mu2oZXGhld9V8lN52",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seque1204/EduceLab/blob/main/DataGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centered_boudaries(image_shape, subarea_size, subareas_per_image):\n",
        "  \"\"\"\n",
        "    Generate a list of bounding boxes centered around the middle of an image.\n",
        "\n",
        "    Args:\n",
        "        image_shape (tuple): A tuple representing the dimensions of the image (height, width, channels).\n",
        "        subarea_size (tuple): A tuple representing the height and width of each subarea.\n",
        "        subareas_per_image (int): The number of subareas to generate within the image.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, each representing the bounding coordinates of a subarea within the image.\n",
        "              Each tuple has the format (top_y, bottom_y, left_x, right_x).\n",
        "\n",
        "    Functionality:\n",
        "        - Calculates the center of the image using its dimensions.\n",
        "        - Defines a standard deviation to control the spread of subarea locations around the center.\n",
        "        - For each subarea, generates top-left coordinates based on a normal distribution centered around\n",
        "          the image's midpoint.\n",
        "        - Clips the coordinates to ensure each subarea is within the image boundaries.\n",
        "        - Returns the list of boundaries, with each subarea centered around the middle, following a normal distribution.\n",
        "\n",
        "    Example:\n",
        "        If image_shape = (200, 200, 3), subarea_size = (50, 50), and subareas_per_image = 5,\n",
        "        the function will return 5 bounding boxes focused around the center of the image, with each box\n",
        "        sized 50x50 pixels.\n",
        "  \"\"\"\n",
        "\n",
        "  height, width, channel = image_shape\n",
        "\n",
        "  # Define the mean and normal distribution (center of image)\n",
        "  mean_x = width // 2\n",
        "  mean_y = height // 2\n",
        "\n",
        "  # Define the standard deviation (smaller values will focus more on center)\n",
        "  std_x = width // 8\n",
        "  std_y = height // 8\n",
        "\n",
        "  boundaries_list = []\n",
        "\n",
        "  for _ in range(subareas_per_image):\n",
        "    # Generate random values from the normal distribution\n",
        "    top_left_x = int(np.clip(np.random.normal(loc = mean_x, scale = std_x), 0, width - subarea_size[1]))\n",
        "    top_left_y = int(np.clip(np.random.normal(loc = mean_y, scale = std_y), 0, height - subarea_size[0]))\n",
        "    boundaries_list.append((top_left_y, top_left_y + subarea_size[0], top_left_x, top_left_x + subarea_size[1]))\n",
        "  return boundaries_list"
      ],
      "metadata": {
        "id": "NhEUUtcDxNPX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_subarea(image, boundaries):\n",
        "  \"\"\"\n",
        "    Extract a subarea from an image based on given boundary coordinates.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array): The input image array from which a subarea will be extracted.\n",
        "                             Expected shape is (height, width, channels).\n",
        "        boundaries (tuple): A tuple representing the coordinates of the subarea to extract.\n",
        "                            The format should be (top_left_y, bottom_right_y, top_left_x, bottom_right_x),\n",
        "                            where:\n",
        "                              - top_left_y: The top boundary row index.\n",
        "                              - bottom_right_y: The bottom boundary row index.\n",
        "                              - top_left_x: The left boundary column index.\n",
        "                              - bottom_right_x: The right boundary column index.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The extracted subarea as a slice of the original image, retaining\n",
        "                     the same number of channels.\n",
        "\n",
        "    Functionality:\n",
        "        - Slices the input image array according to the specified boundaries.\n",
        "        - Returns only the region within the defined boundaries, with the same number\n",
        "          of color channels as the input image.\n",
        "\n",
        "    Example:\n",
        "        If boundaries = (50, 100, 30, 80), the function will return the portion of the\n",
        "        image from row 50 to 100 and column 30 to 80.\n",
        "    \"\"\"\n",
        "\n",
        "  top_left_y, bottom_right_y, top_left_x, bottom_right_x = boundaries\n",
        "  subarea = image[top_left_y:bottom_right_y, top_left_x:bottom_right_x, :]\n",
        "  return subarea"
      ],
      "metadata": {
        "id": "mgThJMId1qa5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m2x6Kp1uwTIP"
      },
      "outputs": [],
      "source": [
        "def subarea_generator(data_list, subarea_size=(32,32),subareas_per_image = 100):\n",
        "  for data_item in data_list:\n",
        "    #Extract image and label\n",
        "    multi_channel_image = data_item['image_data']\n",
        "    image_class = data_item['class']\n",
        "\n",
        "    #generate centered boundaries\n",
        "    boundaries_list = get_centered_boudaries(multi_channel_image.shape, subarea_size, subareas_per_image)\n",
        "\n",
        "    #Yield each subarea and class as tensors\n",
        "    for boundaries in boundaries_list:\n",
        "      subarea = extract_subarea(multi_channel_image, boundaries)\n",
        "      yield subarea, image_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data_list, subarea_size=(32,32),subareas_per_image = 10):\n",
        "  \"\"\"\n",
        "    Create a TensorFlow dataset of image subareas using a generator function.\n",
        "\n",
        "    Args:\n",
        "        data_list (list): A list containing the image data or file paths to images. Each entry corresponds to an image.\n",
        "        subarea_size (tuple, optional): The desired dimensions of each subarea (height, width). Defaults to (32, 32).\n",
        "        subareas_per_image (int, optional): The number of subareas to generate per image. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A TensorFlow dataset where each element is a tuple consisting of:\n",
        "                         - A tensor of shape (subarea_height, subarea_width, 16) representing\n",
        "                           a subarea of the original image.\n",
        "                         - An integer label associated with the subarea, of type int32.\n",
        "  \"\"\"\n",
        "  dataset = tf.data.Dataset.from_generator(\n",
        "      lambda: subarea_generator(data_list, subarea_size, subareas_per_image),\n",
        "      output_signature=(\n",
        "          # The 16 here is the number of channels.\n",
        "          tf.TensorSpec(shape=(subarea_size[0], subarea_size[1], 16), dtype=tf.float32),\n",
        "          tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "_8WDwBjk2qYH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_data_list(num_images=10, image_shape=(64, 64, 16)):\n",
        "    \"\"\"\n",
        "    Generate a list of images with specified dimensions, colors, and classes.\n",
        "\n",
        "    Args:\n",
        "        num_images (int): Total number of images to generate.\n",
        "        image_shape (tuple): Dimensions of each image in (height, width, channels).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary contains:\n",
        "              - 'image_data': The image array filled with a specific color.\n",
        "              - 'class': The class label based on color.\n",
        "              - 'filename': A unique filename for each image.\n",
        "    \"\"\"\n",
        "    # Define color vectors for each class\n",
        "    colors = {\n",
        "        1: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Red\n",
        "        2: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Green\n",
        "        3: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Blue\n",
        "        4: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   # Yellow\n",
        "    }\n",
        "\n",
        "    # Initialize data list\n",
        "    data_list = []\n",
        "\n",
        "    # Calculate the number of images per color\n",
        "    images_per_color = num_images // len(colors)\n",
        "\n",
        "    # Generate images\n",
        "    for class_id, color_vector in colors.items():\n",
        "        for i in range(images_per_color):\n",
        "            image_data = np.tile(color_vector, (image_shape[0], image_shape[1], 1))  # Fill image with color\n",
        "            filename = f\"image_{class_id}_{i+1}.png\"\n",
        "            data_list.append({\n",
        "                'image_data': image_data,\n",
        "                'class': class_id - 1,\n",
        "                'filename': filename\n",
        "            })\n",
        "\n",
        "    return data_list\n"
      ],
      "metadata": {
        "id": "EUtGaHMT3ujw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "data_list = generate_data_list()\n",
        "\n",
        "batch_size = 32\n",
        "subarea_size = (32,32)\n",
        "channels = 16\n",
        "train_dataset = create_dataset(data_list, subarea_size=subarea_size)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#Define a simple CNN model for demo\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(subarea_size[0], subarea_size[1], channels)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')  # 4 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_dataset, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyuH-Qq96u2o",
        "outputId": "b8291e7a-cd18-4cf6-e415-13602777552d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.6109 - loss: 1.2394\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.6817\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.2561\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0569\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0058\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.9462e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.7613e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.5008e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 5.4039e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.3904e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d85d5104580>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CdH6SbiW61d3"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}